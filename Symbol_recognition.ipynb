{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функционал обработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "from spacy.lang import ru\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop = set(nltk.corpus.stopwords.words('russian'))\n",
    "Unused_chars=[':',',','.','-','\\n','?','!']\n",
    "stemmer = SnowballStemmer('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def tokenize(file_text):\n",
    "    tokens = word_tokenize(file_text)\n",
    "    tokens = [i for i in tokens if (i not in stop)]\n",
    "    tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение имен и именованных сущностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Одиночные вхождения имени, фамилии, отчества\n",
    "from natasha import NamesExtractor\n",
    "\n",
    "\n",
    "def Name_recognition(text):\n",
    "    Names=[]\n",
    "    Families=[]\n",
    "    Lasts=[]\n",
    "    custom_useless_words=[]\n",
    "    #custom_useless_words=['из-за','марка','ведь','патриарший','борменталь','филипп','кихот','санчо','антоний','сансон','шарик','мой','такой','другой','ваш','этот','сам','свой','тот','варенуха','степ','маргарита','иван','воланд','коровьев','пилат','николай','какой-то','свой','самый','быть','стать','это','что-то','весь','какой','мочь','никанор','который','ты','никакой','азазело','азазелло','берлиоз']\n",
    "    extractor = NamesExtractor()\n",
    "    matches = extractor(text)\n",
    "    for match in matches:\n",
    "        if match.fact.first is not None:\n",
    "            tokens=prepare_text_for_lda(match.fact.first)\n",
    "            if len(tokens)>0:\n",
    "                Names.append(tokens[0])\n",
    "        if match.fact.last is not None:\n",
    "            tokens=prepare_text_for_lda(match.fact.last)\n",
    "            if len(tokens)>0:\n",
    "                Families.append(tokens[0])\n",
    "        if match.fact.middle is not None:\n",
    "            tokens=prepare_text_for_lda(match.fact.middle)\n",
    "            if len(tokens)>0:\n",
    "                Lasts.append(tokens[0])\n",
    "    Names_set=set(Names)\n",
    "    Fam_set=set(Families)\n",
    "    Lasts_set=set(Lasts)\n",
    "    useless_set=set(custom_useless_words)\n",
    "    for name in Names_set:\n",
    "        useless_set.add(name)\n",
    "    for name in Fam_set:\n",
    "        useless_set.add(name)\n",
    "    for name in Lasts_set:\n",
    "        useless_set.add(name)\n",
    "    return useless_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Работа с DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_value=0.75\n",
    "\n",
    "Unused_chars=[':',',','.','-','\\n','?','!']\n",
    "\n",
    "#Логический разделитель\n",
    "Logic_Sep=['PREP','CONJ','PRCL']\n",
    "Logic_AdVerb=['PRTF','PRTS','GRND']\n",
    "Logic_Adj=['ADJF','ADJS']\n",
    "verb=['VERB','INFN']\n",
    "Separators=Logic_Sep+Logic_AdVerb+verb+Logic_Adj\n",
    "\n",
    "def logic_construct_extractor(string):\n",
    "    Main_noun=''\n",
    "    verbs=[]\n",
    "    temp_string=''\n",
    "    state=False\n",
    "    Logical_structure=[]\n",
    "    for word in string.split():\n",
    "        f_word=word\n",
    "        for char in Unused_chars:\n",
    "            if char in f_word:\n",
    "                f_word=f_word.replace(char,'')\n",
    "        p=morph.parse(f_word)[0]\n",
    "        if p.tag.POS in Separators and len(temp_string)>0 and not state:\n",
    "            Logical_structure.append(temp_string)\n",
    "            temp_string=f_word+' '\n",
    "            state=True\n",
    "        else:\n",
    "            temp_string+=f_word+' '\n",
    "            state=False\n",
    "    Logical_structure.append(temp_string)\n",
    "    return Logical_structure\n",
    "\n",
    "def check_symbol_string(string,decision_value):\n",
    "    count=0\n",
    "    ngramm=string.split()\n",
    "    res=decision_value*len(ngramm)\n",
    "    for gramm in ngramm:\n",
    "        tokens=prepare_text_for_lda(gramm)\n",
    "        if not(len(tokens)>0 and (tokens[0] not in useless_set)):\n",
    "            continue\n",
    "        for symbol in symbols:\n",
    "            if tokens[0] in symbol:\n",
    "                count+=1\n",
    "                break\n",
    "    if (count>=res)and len(ngramm)>1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_file_symbol(filename,decision_value):\n",
    "    count_change=0\n",
    "    count_symbols_var=0\n",
    "    prev_value=False\n",
    "    ngramm=[]\n",
    "    result={}\n",
    "    with open('Lyrics\\\\'+filename+'.txt',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not len(line)>1:\n",
    "                continue\n",
    "            for string in line.split('.'):\n",
    "                logic_structs=logic_construct_extractor(string)\n",
    "                for logic_struct in logic_structs:\n",
    "                    if check_symbol_string(logic_struct,decision_value):\n",
    "                        result[logic_struct]=string\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob('Data//*.csv'):\n",
    "    print(filename)\n",
    "    with open(filename) as csvfile:\n",
    "        input_file1 = csv.DictReader(csvfile,delimiter=\";\")\n",
    "        input_file=list(input_file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from docx.shared import Pt\n",
    "from docx.enum.style import WD_STYLE_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some=check_file_symbol('Chapter2',0.75)\n",
    "\n",
    "print('ready')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
